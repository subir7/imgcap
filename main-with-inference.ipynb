{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkENNah08hJK",
        "outputId": "7f23a957-08d2-4003-94e2-94de23442ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/MSCOCO/Flicker8k_Dataset.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "TQqNaKenspyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bcbX2GkBse4B",
        "outputId": "60117851-deb0-4803-d25c-9c7c8d4db8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL ARCHITECTURE - 180D MSCOCO FEATURES\n",
            "Input 1: Image (224x224x3)\n",
            "Input 2: Context vector (180D)\n",
            "  - Objects: 80D (person, car, dog, ...)\n",
            "  - Stuff: 91D (sky, grass, water, beach, ...)\n",
            "  - Scene stats: 9D\n",
            "LOADING DATA\n",
            "Loaded 40460 captions\n",
            "LOADING 180D MSCOCO FEATURES\n",
            "Loading from: /content/drive/MyDrive/MSCOCO/trainingdata/mscoco_object_stuff_detection.csv\n",
            "  Loaded MSCOCO features\n",
            "  Rows: 8091\n",
            "  Columns: 181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-757765191.py:141: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  filename = row[0] if isinstance(row[0], str) else row['filename']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 8091 images\n",
            "\n",
            "Sample verification:\n",
            "  Filename: 2306674172_dc07c7f847.jpg\n",
            "  Feature vector length: 180\n",
            "  Non-zero features: 24\n",
            "  Feature range: [0.000, 9.000]\n",
            "CREATING CONTEXT VECTORS\n",
            "Context vectors created: 40460\n",
            "Context dimension: 180\n",
            "\n",
            "Data filtering:\n",
            "  Initial captions: 40460\n",
            "  Complete data: 40460\n",
            "  Filtered out: 0\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING CAPTIONS\n",
            "================================================================================\n",
            "Vocabulary size: 8832\n",
            "Caption sequences created\n",
            "\n",
            "Sample caption:\n",
            "  Original: startseq a child in a pink dress is climbing up a set of stairs in an entry way endseq\n",
            "  Sequence length: 40\n",
            "\n",
            " Dataset pipeline created\n",
            "BUILDING MODEL\n",
            "\n",
            " Model created\n",
            "  Encoder: 6 layers\n",
            "  Decoder: 6 layers\n",
            "  Context: 180D (180D MSCOCO)\n",
            "  Vocabulary: 8832 words\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ vit_encoder         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │  \u001b[38;5;34m3,409,664\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sequence_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context_fusion      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m46,848\u001b[0m │ vit_encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mContextFusion\u001b[0m)     │                   │            │ context_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m8832\u001b[0m)  │  \u001b[38;5;34m9,275,520\u001b[0m │ sequence_input[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ context_fusion[\u001b[38;5;34m0\u001b[0m… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ vit_encoder         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,409,664</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ sequence_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context_fusion      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">46,848</span> │ vit_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ContextFusion</span>)     │                   │            │ context_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8832</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,275,520</span> │ sequence_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ context_fusion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,732,032\u001b[0m (48.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,732,032</span> (48.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,732,032\u001b[0m (48.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,732,032</span> (48.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKING FOR PREVIOUS TRAINING\n",
            "\n",
            " Found previous training:\n",
            "  Stage: 2\n",
            "  Completed epochs: 4\n",
            "Training already complete!\n",
            "SAVING FINAL MODEL\n",
            "Model weights: /content/drive/MyDrive/MSCOCO/trainingdata/final_model.weights.h5\n",
            "Tokenizer: /content/drive/MyDrive/MSCOCO/trainingdata/tokenizer.pkl\n",
            "Configuration: /content/drive/MyDrive/MSCOCO/trainingdata/model_config.pkl\n",
            "Training history: /content/drive/MyDrive/MSCOCO/trainingdata/training_history.pkl\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE!\n",
            "================================================================================\n",
            "Architecture: Image + 180D MSCOCO Context\n",
            "  • 80D: Objects (person, car, dog, ...)\n",
            "  • 91D: Stuff (sky, grass, water, beach, ...)\n",
            "  • 9D: Scene statistics\n",
            "Total epochs: 9\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "\n",
        "# ---------------- HYPERPARAMETERS ----------------\n",
        "IMG_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "EMBED_DIM = 256\n",
        "NUM_HEADS = 4\n",
        "MLP_DIM = 512\n",
        "\n",
        "ENCODER_LAYERS = 6\n",
        "DECODER_LAYERS = 6\n",
        "\n",
        "MAX_LEN = 40\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS_STAGE1 = 5 #20\n",
        "EPOCHS_STAGE2 = 4 #15\n",
        "\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# ============================================================================\n",
        "# 180D MSCOCO FEATURE DIMENSIONS\n",
        "# ============================================================================\n",
        "# Complete MSCOCO features:\n",
        "# - 80D: Object detection (person, car, dog, etc.)\n",
        "# - 91D: Stuff detection (sky, grass, water, beach, etc.)\n",
        "# - 9D: Scene statistics\n",
        "MSCOCO_OBJECTS_DIM = 80\n",
        "MSCOCO_STUFF_DIM = 91\n",
        "SCENE_STATS_DIM = 9\n",
        "\n",
        "# Total context dimension\n",
        "CONTEXT_DIM = MSCOCO_OBJECTS_DIM + MSCOCO_STUFF_DIM + SCENE_STATS_DIM  # 180\n",
        "\n",
        "\n",
        "print(\"MODEL ARCHITECTURE - 180D MSCOCO FEATURES\")\n",
        "\n",
        "print(f\"Input 1: Image ({IMG_SIZE}x{IMG_SIZE}x3)\")\n",
        "print(f\"Input 2: Context vector ({CONTEXT_DIM}D)\")\n",
        "print(f\"  - Objects: {MSCOCO_OBJECTS_DIM}D (person, car, dog, ...)\")\n",
        "print(f\"  - Stuff: {MSCOCO_STUFF_DIM}D (sky, grass, water, beach, ...)\")\n",
        "print(f\"  - Scene stats: {SCENE_STATS_DIM}D\")\n",
        "\n",
        "# ---------------- PATHS ----------------\n",
        "CAPTION_FILE = \"/content/Flickr8k_text/Flickr8k.token.txt\"\n",
        "IMG_DIR = \"/content/Flicker8k_Dataset/\"\n",
        "\n",
        "# NEW: 180D MSCOCO features CSV\n",
        "MSCOCO_FEATURES_FILE = \"/content/drive/MyDrive/MSCOCO/trainingdata/mscoco_object_stuff_detection.csv\"\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/MSCOCO/trainingdata/checkpoints\"\n",
        "STAGE1_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \"stage1_latest.weights.h5\")\n",
        "STAGE2_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \"stage2_latest.weights.h5\")\n",
        "EPOCH_TRACKER = os.path.join(CHECKPOINT_DIR, \"training_progress.pkl\")\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# CHECKPOINT MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "def save_training_progress(stage, epoch, history_dict):\n",
        "    progress = {\n",
        "        'stage': stage,\n",
        "        'completed_epochs': epoch,\n",
        "        'history': history_dict\n",
        "    }\n",
        "    with open(EPOCH_TRACKER, 'wb') as f:\n",
        "        pickle.dump(progress, f)\n",
        "    print(f\"\\n Saved progress: Stage {stage}, Epoch {epoch}\")\n",
        "\n",
        "def load_training_progress():\n",
        "    if os.path.exists(EPOCH_TRACKER):\n",
        "        with open(EPOCH_TRACKER, 'rb') as f:\n",
        "            progress = pickle.load(f)\n",
        "        print(f\"\\n Found previous training:\")\n",
        "        print(f\"  Stage: {progress['stage']}\")\n",
        "        print(f\"  Completed epochs: {progress['completed_epochs']}\")\n",
        "        return progress\n",
        "    return None\n",
        "\n",
        "def get_latest_checkpoint(stage):\n",
        "    if stage == 1:\n",
        "        if os.path.exists(STAGE1_CHECKPOINT):\n",
        "            return STAGE1_CHECKPOINT\n",
        "    elif stage == 2:\n",
        "        if os.path.exists(STAGE2_CHECKPOINT):\n",
        "            return STAGE2_CHECKPOINT\n",
        "    return None\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "print(\"LOADING DATA\")\n",
        "\n",
        "# Load captions\n",
        "df = pd.read_csv(CAPTION_FILE, sep='\\t', header=None, names=[\"image_id\", \"caption\"])\n",
        "df[\"image_id\"] = df[\"image_id\"].apply(lambda x: x.split(\"#\")[0])\n",
        "df = df.rename(columns={\"image_id\": \"filename\"})\n",
        "print(f\"Loaded {len(df)} captions\")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD 180D MSCOCO FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"LOADING 180D MSCOCO FEATURES\")\n",
        "\n",
        "if not os.path.exists(MSCOCO_FEATURES_FILE):\n",
        "    print(f\" ERROR: MSCOCO features file not found!\")\n",
        "    print(f\"   Expected: {MSCOCO_FEATURES_FILE}\")\n",
        "    print(\"\\nYou need to run the feature extraction script first:\")\n",
        "    print(\"   python mscoco_complete_extractor.py\")\n",
        "    print(\"\\nThis will generate the 180D features CSV file.\")\n",
        "    exit(1)\n",
        "\n",
        "print(f\"Loading from: {MSCOCO_FEATURES_FILE}\")\n",
        "mscoco_df = pd.read_csv(MSCOCO_FEATURES_FILE)\n",
        "\n",
        "print(f\"  Loaded MSCOCO features\")\n",
        "print(f\"  Rows: {len(mscoco_df)}\")\n",
        "print(f\"  Columns: {len(mscoco_df.columns)}\")\n",
        "\n",
        "# Expected format: filename, feat_0, feat_1, ..., feat_179\n",
        "# Total: 181 columns (1 filename + 180 features)\n",
        "\n",
        "if len(mscoco_df.columns) < 181:\n",
        "    print(f\"\\n WARNING: Expected 181 columns (1 filename + 180 features)\")\n",
        "    print(f\"   Found: {len(mscoco_df.columns)} columns\")\n",
        "    print(f\"   Will pad with zeros if needed\")\n",
        "\n",
        "# Extract features into dictionary\n",
        "mscoco_data = {}\n",
        "\n",
        "for _, row in mscoco_df.iterrows():\n",
        "    filename = row[0] if isinstance(row[0], str) else row['filename']\n",
        "\n",
        "    # Extract 180 features (columns 1-180)\n",
        "    if len(row) >= 181:\n",
        "        features_180d = row[1:181].values.astype(np.float32).tolist()\n",
        "    else:\n",
        "        # Pad if needed\n",
        "        available_features = row[1:].values.astype(np.float32).tolist()\n",
        "        padding = [0.0] * (CONTEXT_DIM - len(available_features))\n",
        "        features_180d = available_features + padding\n",
        "\n",
        "    mscoco_data[filename] = features_180d\n",
        "\n",
        "print(f\"Processed {len(mscoco_data)} images\")\n",
        "\n",
        "# Sample verification\n",
        "sample_file = list(mscoco_data.keys())[0]\n",
        "sample_features = mscoco_data[sample_file]\n",
        "print(f\"\\nSample verification:\")\n",
        "print(f\"  Filename: {sample_file}\")\n",
        "print(f\"  Feature vector length: {len(sample_features)}\")\n",
        "print(f\"  Non-zero features: {np.count_nonzero(sample_features)}\")\n",
        "print(f\"  Feature range: [{min(sample_features):.3f}, {max(sample_features):.3f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE CONTEXT VECTORS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"CREATING CONTEXT VECTORS\")\n",
        "\n",
        "def create_context_vector(filename):\n",
        "    \"\"\"\n",
        "    Get 180D MSCOCO features as context vector\n",
        "\n",
        "    Returns: 180D vector\n",
        "      - [0:80] = Object features (person, car, dog, ...)\n",
        "      - [80:171] = Stuff features (sky, grass, water, beach, ...)\n",
        "      - [171:180] = Scene statistics\n",
        "    \"\"\"\n",
        "    return mscoco_data.get(filename, [0.0] * CONTEXT_DIM)\n",
        "\n",
        "# Add context vector to dataframe\n",
        "df[\"context\"] = df[\"filename\"].apply(create_context_vector)\n",
        "\n",
        "# Verify context vectors\n",
        "print(f\"Context vectors created: {len(df)}\")\n",
        "print(f\"Context dimension: {len(df.iloc[0]['context'])}\")\n",
        "\n",
        "# Filter complete data\n",
        "initial_count = len(df)\n",
        "df = df[df[\"context\"].apply(lambda x: len(x) == CONTEXT_DIM)]\n",
        "final_count = len(df)\n",
        "\n",
        "print(f\"\\nData filtering:\")\n",
        "print(f\"  Initial captions: {initial_count}\")\n",
        "print(f\"  Complete data: {final_count}\")\n",
        "print(f\"  Filtered out: {initial_count - final_count}\")\n",
        "\n",
        "if final_count == 0:\n",
        "    print(\"\\n ERROR: No valid data found!\")\n",
        "    print(\"   Check that filenames match between caption file and MSCOCO features CSV\")\n",
        "    exit(1)\n",
        "\n",
        "# Text preprocessing\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPROCESSING CAPTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def clean_caption(c):\n",
        "    c = c.lower()\n",
        "    c = re.sub(r\"[^a-z0-9 ]\", \"\", c)\n",
        "    return \"startseq \" + c.strip() + \" endseq\"\n",
        "\n",
        "df[\"caption\"] = df[\"caption\"].apply(clean_caption)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(df[\"caption\"].values)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = len(word_index) + 1\n",
        "\n",
        "seqs = tokenizer.texts_to_sequences(df[\"caption\"])\n",
        "seqs = tf.keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\")\n",
        "df[\"seq\"] = seqs.tolist()\n",
        "\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"Caption sequences created\")\n",
        "\n",
        "# Sample caption\n",
        "sample_idx = 0\n",
        "print(f\"\\nSample caption:\")\n",
        "print(f\"  Original: {df.iloc[sample_idx]['caption']}\")\n",
        "print(f\"  Sequence length: {len(df.iloc[sample_idx]['seq'])}\")\n",
        "\n",
        "# Image loader\n",
        "def load_image(path):\n",
        "    try:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "        arr = np.array(img).astype(np.float32) / 255.0\n",
        "        return arr\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# ============================================================================\n",
        "# DATA GENERATOR - 2 inputs + sequence\n",
        "# ============================================================================\n",
        "\n",
        "def data_generator():\n",
        "    \"\"\"Yields ((image, context), seq_target)\"\"\"\n",
        "    for _, row in df.iterrows():\n",
        "        filename = row[\"filename\"]\n",
        "        img_path = os.path.join(IMG_DIR, filename)\n",
        "\n",
        "        if not os.path.exists(img_path) and img_path.endswith(\".jpg.1\"):\n",
        "            candidate = img_path.replace(\".jpg.1\", \".jpg\")\n",
        "            if os.path.exists(candidate):\n",
        "                img_path = candidate\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            continue\n",
        "\n",
        "        img = load_image(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        seq = np.array(row[\"seq\"], dtype=np.int32)\n",
        "        inp = seq[:-1]\n",
        "        out = seq[1:]\n",
        "\n",
        "        if inp.shape[0] != MAX_LEN - 1 or out.shape[0] != MAX_LEN - 1:\n",
        "            continue\n",
        "\n",
        "        context = np.array(row[\"context\"], dtype=np.float32)\n",
        "\n",
        "        yield (img, context, inp), out\n",
        "\n",
        "output_signature = (\n",
        "    (tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n",
        "     tf.TensorSpec(shape=(CONTEXT_DIM,), dtype=tf.float32),\n",
        "     tf.TensorSpec(shape=(MAX_LEN-1,), dtype=tf.int32)),\n",
        "    tf.TensorSpec(shape=(MAX_LEN-1,), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(data_generator, output_signature=output_signature)\n",
        "dataset = dataset.shuffle(2048).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"\\n Dataset pipeline created\")\n",
        "\n",
        "# ---------------- MODEL COMPONENTS ----------------\n",
        "\n",
        "class PatchExtract(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [tf.shape(images)[0], -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patches, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos = layers.Embedding(num_patches, embed_dim)\n",
        "\n",
        "    def call(self, patches):\n",
        "        pos_ids = tf.range(start=0, limit=NUM_PATCHES, delta=1)\n",
        "        pos_emb = self.pos(pos_ids)\n",
        "        pos_emb = tf.expand_dims(pos_emb, axis=0)\n",
        "        x = self.proj(patches) + pos_emb\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            layers.Dense(mlp_dim, activation=\"gelu\"),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x):\n",
        "        att_out = self.att(x, x)\n",
        "        x = self.ln1(x + att_out)\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + ff_out)\n",
        "        return x\n",
        "\n",
        "def build_vit_encoder():\n",
        "    img = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    patches = PatchExtract(PATCH_SIZE)(img)\n",
        "    x = PatchEmbedding(NUM_PATCHES, EMBED_DIM)(patches)\n",
        "\n",
        "    for _ in range(ENCODER_LAYERS):\n",
        "        x = EncoderBlock(EMBED_DIM, NUM_HEADS, MLP_DIM)(x)\n",
        "\n",
        "    return tf.keras.Model(img, x, name=\"vit_encoder\")\n",
        "\n",
        "class ContextFusion(layers.Layer):\n",
        "    \"\"\"Fuses 180D MSCOCO context with visual features\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.context_proj = layers.Dense(embed_dim, name=\"context_projection\")\n",
        "        self.dropout = layers.Dropout(DROPOUT)\n",
        "        self.ln = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, enc_out, context, training=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            enc_out: (batch, NUM_PATCHES, embed_dim) - visual features\n",
        "            context: (batch, 180) - MSCOCO features (80 objects + 91 stuff + 9 stats)\n",
        "\n",
        "        Returns:\n",
        "            (batch, NUM_PATCHES + 1, embed_dim) - fused features\n",
        "        \"\"\"\n",
        "        context_emb = self.context_proj(context)\n",
        "        context_emb = self.dropout(context_emb, training=training)\n",
        "        context_emb = tf.expand_dims(context_emb, 1)\n",
        "\n",
        "        combined = tf.concat([enc_out, context_emb], axis=1)\n",
        "        return self.ln(combined)\n",
        "\n",
        "class DecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.self_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "        self.cross_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dropout(DROPOUT),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ln3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(DROPOUT)\n",
        "        self.dropout2 = layers.Dropout(DROPOUT)\n",
        "\n",
        "    def _causal_mask(self, seq_len, batch_size):\n",
        "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        mask = tf.cast(mask, tf.bool)\n",
        "        mask = tf.reshape(mask, (1, seq_len, seq_len))\n",
        "        return tf.repeat(mask, repeats=batch_size, axis=0)\n",
        "\n",
        "    def call(self, x, enc_out, training=False):\n",
        "        batch = tf.shape(x)[0]\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        mask = self._causal_mask(seq_len, batch)\n",
        "\n",
        "        att1 = self.self_att(x, x, attention_mask=mask)\n",
        "        att1 = self.dropout1(att1, training=training)\n",
        "        x = self.ln1(x + att1)\n",
        "\n",
        "        att2 = self.cross_att(x, enc_out)\n",
        "        att2 = self.dropout2(att2, training=training)\n",
        "        x = self.ln2(x + att2)\n",
        "\n",
        "        f = self.ff(x, training=training)\n",
        "        x = self.ln3(x + f)\n",
        "        return x\n",
        "\n",
        "def build_decoder():\n",
        "    seq_in = layers.Input(shape=(MAX_LEN-1,), dtype=tf.int32)\n",
        "    enc_out = layers.Input(shape=(NUM_PATCHES + 1, EMBED_DIM))\n",
        "\n",
        "    tok_emb = layers.Embedding(VOCAB_SIZE, EMBED_DIM)(seq_in)\n",
        "    pos_ids = tf.range(start=0, limit=MAX_LEN-1, delta=1)\n",
        "    pos_layer = layers.Embedding(MAX_LEN, EMBED_DIM)\n",
        "    pos_emb = pos_layer(pos_ids)\n",
        "    pos_emb = tf.expand_dims(pos_emb, 0)\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    for _ in range(DECODER_LAYERS):\n",
        "        x = DecoderBlock(EMBED_DIM, MLP_DIM, NUM_HEADS)(x, enc_out)\n",
        "\n",
        "    out = layers.Dense(VOCAB_SIZE)(x)\n",
        "    return tf.keras.Model([seq_in, enc_out], out, name=\"decoder\")\n",
        "\n",
        "# ============================================================================\n",
        "# BUILD MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"BUILDING MODEL\")\n",
        "\n",
        "encoder = build_vit_encoder()\n",
        "decoder = build_decoder()\n",
        "context_fusion = ContextFusion(EMBED_DIM)\n",
        "\n",
        "img_inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"image_input\")\n",
        "context_inp = layers.Input(shape=(CONTEXT_DIM,), name=\"context_input\")\n",
        "seq_inp = layers.Input(shape=(MAX_LEN-1,), dtype=tf.int32, name=\"sequence_input\")\n",
        "\n",
        "enc_out = encoder(img_inp)\n",
        "fused_out = context_fusion(enc_out, context_inp)\n",
        "dec_out = decoder([seq_inp, fused_out])\n",
        "\n",
        "model = tf.keras.Model([img_inp, context_inp, seq_inp], dec_out)\n",
        "\n",
        "print(f\"\\n Model created\")\n",
        "print(f\"  Encoder: {ENCODER_LAYERS} layers\")\n",
        "print(f\"  Decoder: {DECODER_LAYERS} layers\")\n",
        "print(f\"  Context: {CONTEXT_DIM}D (180D MSCOCO)\")\n",
        "print(f\"  Vocabulary: {VOCAB_SIZE} words\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ============================================================================\n",
        "# CHECKPOINT SYSTEM\n",
        "# ============================================================================\n",
        "\n",
        "print(\"CHECKING FOR PREVIOUS TRAINING\")\n",
        "\n",
        "progress = load_training_progress()\n",
        "\n",
        "if progress is None:\n",
        "    print(\"No previous training found. Starting from scratch.\")\n",
        "    current_stage = 1\n",
        "    stage1_start_epoch = 0\n",
        "    stage2_start_epoch = 0\n",
        "    training_history = {'stage1': {}, 'stage2': {}}\n",
        "else:\n",
        "    current_stage = progress['stage']\n",
        "    completed_epochs = progress['completed_epochs']\n",
        "    training_history = progress.get('history', {'stage1': {}, 'stage2': {}})\n",
        "\n",
        "    if current_stage == 1:\n",
        "        stage1_start_epoch = completed_epochs\n",
        "        stage2_start_epoch = 0\n",
        "\n",
        "        if stage1_start_epoch >= EPOCHS_STAGE1:\n",
        "            current_stage = 2\n",
        "            stage1_start_epoch = EPOCHS_STAGE1\n",
        "            stage2_start_epoch = 0\n",
        "        else:\n",
        "            print(f\"Resuming Stage 1 from epoch {stage1_start_epoch}/{EPOCHS_STAGE1}\")\n",
        "    else:\n",
        "        stage1_start_epoch = EPOCHS_STAGE1\n",
        "        stage2_start_epoch = completed_epochs\n",
        "\n",
        "        if stage2_start_epoch >= EPOCHS_STAGE2:\n",
        "            print(f\"Training already complete!\")\n",
        "            exit(0)\n",
        "        else:\n",
        "            print(f\"Resuming Stage 2 from epoch {stage2_start_epoch}/{EPOCHS_STAGE2}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STAGE 1: TRAIN DECODER ONLY\n",
        "# ============================================================================\n",
        "\n",
        "if current_stage == 1 and stage1_start_epoch < EPOCHS_STAGE1:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"STAGE 1: DECODER TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    encoder.trainable = False\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    enc_out = encoder(img_inp)\n",
        "    fused_out = context_fusion(enc_out, context_inp)\n",
        "    dec_out = decoder([seq_inp, fused_out])\n",
        "    model = tf.keras.Model([img_inp, context_inp, seq_inp], dec_out)\n",
        "\n",
        "    if stage1_start_epoch > 0:\n",
        "        checkpoint = get_latest_checkpoint(1)\n",
        "        if checkpoint:\n",
        "            model.load_weights(checkpoint)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    class EpochCheckpoint(tf.keras.callbacks.Callback):\n",
        "        def __init__(self, stage, start_epoch):\n",
        "            super().__init__()\n",
        "            self.stage = stage\n",
        "            self.start_epoch = start_epoch\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            actual_epoch = self.start_epoch + epoch + 1\n",
        "\n",
        "            if self.stage == 1:\n",
        "                self.model.save_weights(STAGE1_CHECKPOINT)\n",
        "            else:\n",
        "                self.model.save_weights(STAGE2_CHECKPOINT)\n",
        "\n",
        "            save_training_progress(self.stage, actual_epoch, training_history)\n",
        "\n",
        "    epoch_checkpoint = EpochCheckpoint(stage=1, start_epoch=stage1_start_epoch)\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-7)\n",
        "\n",
        "    remaining_epochs = EPOCHS_STAGE1 - stage1_start_epoch\n",
        "\n",
        "    history1 = model.fit(\n",
        "        dataset,\n",
        "        epochs=remaining_epochs,\n",
        "        callbacks=[epoch_checkpoint, early_stop, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    for key, values in history1.history.items():\n",
        "        if key not in training_history['stage1']:\n",
        "            training_history['stage1'][key] = []\n",
        "        training_history['stage1'][key].extend(values)\n",
        "\n",
        "    current_stage = 2\n",
        "    stage2_start_epoch = 0\n",
        "\n",
        "# ============================================================================\n",
        "# STAGE 2: FINE-TUNE ENTIRE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "if current_stage == 2 and stage2_start_epoch < EPOCHS_STAGE2:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"STAGE 2: FINE-TUNING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    encoder.trainable = True\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    enc_out = encoder(img_inp)\n",
        "    fused_out = context_fusion(enc_out, context_inp)\n",
        "    dec_out = decoder([seq_inp, fused_out])\n",
        "    model = tf.keras.Model([img_inp, context_inp, seq_inp], dec_out)\n",
        "\n",
        "    if stage2_start_epoch == 0:\n",
        "        checkpoint = get_latest_checkpoint(1)\n",
        "        if checkpoint:\n",
        "            model.load_weights(checkpoint)\n",
        "    else:\n",
        "        checkpoint = get_latest_checkpoint(2)\n",
        "        if checkpoint:\n",
        "            model.load_weights(checkpoint)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    epoch_checkpoint = EpochCheckpoint(stage=2, start_epoch=stage2_start_epoch)\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-7)\n",
        "\n",
        "    remaining_epochs = EPOCHS_STAGE2 - stage2_start_epoch\n",
        "\n",
        "    history2 = model.fit(\n",
        "        dataset,\n",
        "        epochs=remaining_epochs,\n",
        "        callbacks=[epoch_checkpoint, early_stop, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    for key, values in history2.history.items():\n",
        "        if key not in training_history['stage2']:\n",
        "            training_history['stage2'][key] = []\n",
        "        training_history['stage2'][key].extend(values)\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE FINAL MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"SAVING FINAL MODEL\")\n",
        "\n",
        "final_weights = \"/content/drive/MyDrive/MSCOCO/trainingdata/final_model.weights.h5\"\n",
        "model.save_weights(final_weights)\n",
        "\n",
        "tokenizer_path = \"/content/drive/MyDrive/MSCOCO/trainingdata/tokenizer.pkl\"\n",
        "with open(tokenizer_path, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "config = {\n",
        "    'IMG_SIZE': IMG_SIZE,\n",
        "    'PATCH_SIZE': PATCH_SIZE,\n",
        "    'NUM_PATCHES': NUM_PATCHES,\n",
        "    'EMBED_DIM': EMBED_DIM,\n",
        "    'NUM_HEADS': NUM_HEADS,\n",
        "    'MLP_DIM': MLP_DIM,\n",
        "    'ENCODER_LAYERS': ENCODER_LAYERS,\n",
        "    'DECODER_LAYERS': DECODER_LAYERS,\n",
        "    'MAX_LEN': MAX_LEN,\n",
        "    'CONTEXT_DIM': CONTEXT_DIM,\n",
        "    'MSCOCO_OBJECTS_DIM': MSCOCO_OBJECTS_DIM,\n",
        "    'MSCOCO_STUFF_DIM': MSCOCO_STUFF_DIM,\n",
        "    'SCENE_STATS_DIM': SCENE_STATS_DIM,\n",
        "    'VOCAB_SIZE': VOCAB_SIZE\n",
        "}\n",
        "\n",
        "config_path = \"/content/drive/MyDrive/MSCOCO/trainingdata/model_config.pkl\"\n",
        "with open(config_path, 'wb') as f:\n",
        "    pickle.dump(config, f)\n",
        "\n",
        "history_path = \"/content/drive/MyDrive/MSCOCO/trainingdata/training_history.pkl\"\n",
        "with open(history_path, 'wb') as f:\n",
        "    pickle.dump(training_history, f)\n",
        "\n",
        "print(f\"Model weights: {final_weights}\")\n",
        "print(f\"Tokenizer: {tokenizer_path}\")\n",
        "print(f\"Configuration: {config_path}\")\n",
        "print(f\"Training history: {history_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Architecture: Image + 180D MSCOCO Context\")\n",
        "print(f\"  • 80D: Objects (person, car, dog, ...)\")\n",
        "print(f\"  • 91D: Stuff (sky, grass, water, beach, ...)\")\n",
        "print(f\"  • 9D: Scene statistics\")\n",
        "print(f\"Total epochs: {EPOCHS_STAGE1 + EPOCHS_STAGE2}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AR9naDU5XKwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model as a `.keras` zip archive.\n",
        "model.save('/content/drive/MyDrive/MSCOCO/trainingdata/final_model.keras')\n",
        "model.save('/content/drive/MyDrive/MSCOCO/trainingdata/final_model.h5')"
      ],
      "metadata": {
        "id": "XkBtTfQvXKzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3664c3b7-bbb2-4627-c802-9e3fe99fa823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IgzotTqmXK2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-HcUN50xXK6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INFERENCE"
      ],
      "metadata": {
        "id": "KUjSre4j-EgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "COMPLETE AUTOMATIC INFERENCE SCRIPT - 180D MSCOCO Features\n",
        "Input: Image file\n",
        "Output: Generated caption\n",
        "\n",
        "Uses 180D MSCOCO features:\n",
        "- 80D: Object detection (person, car, dog, etc.)\n",
        "- 91D: Stuff detection (sky, grass, water, etc.)\n",
        "- 9D: Scene statistics\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"AUTOMATIC IMAGE CAPTIONING - 180D MSCOCO Features\")\n",
        "print(\"Loading models and dependencies...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# MSCOCO CLASS DEFINITIONS\n",
        "# ============================================================================\n",
        "\n",
        "MSCOCO_OBJECTS = [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
        "    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
        "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n",
        "    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
        "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "MSCOCO_STUFF = [\n",
        "    'sky', 'grass', 'tree', 'mountain', 'hill', 'rock', 'water', 'sea', 'river', 'lake',\n",
        "    'sand', 'snow', 'fog', 'clouds', 'bush', 'flower', 'leaves', 'branch', 'dirt', 'mud',\n",
        "    'building', 'house', 'bridge', 'fence', 'wall', 'roof', 'door', 'window', 'stairs',\n",
        "    'ceiling', 'floor', 'platform', 'pavement', 'road', 'railroad', 'ground',\n",
        "    'cabinet', 'shelf', 'table', 'counter', 'carpet', 'rug', 'curtain', 'blanket',\n",
        "    'pillow', 'towel', 'mirror', 'light', 'paper', 'cardboard', 'wood', 'metal',\n",
        "    'plastic', 'glass', 'tile', 'brick', 'stone',\n",
        "    'banner', 'net', 'tent', 'playingfield', 'fruit', 'vegetable', 'food', 'cloth',\n",
        "    'textile', 'plant', 'gravel', 'moss', 'straw',\n",
        "    # Pad to 91 classes\n",
        "    'material', 'surface', 'landscape', 'scenery', 'background', 'foreground',\n",
        "    'area', 'region', 'space', 'field', 'place', 'location', 'terrain', 'zone',\n",
        "    'environment', 'setting', 'context', 'atmosphere', 'element', 'component',\n",
        "    'structure', 'formation'\n",
        "][:91]  # Ensure exactly 91\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD MODEL CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG_PATH = \"/content/drive/MyDrive/MSCOCO/trainingdata/model_config.pkl\"\n",
        "WEIGHTS_PATH = \"/content/drive/MyDrive/MSCOCO/trainingdata/final_model.weights.h5\"\n",
        "TOKENIZER_PATH = \"/content/drive/MyDrive/MSCOCO/trainingdata/tokenizer.pkl\"\n",
        "\n",
        "print(\"\\n[1/5] Loading configuration...\")\n",
        "with open(CONFIG_PATH, 'rb') as f:\n",
        "    config = pickle.load(f)\n",
        "\n",
        "IMG_SIZE = config['IMG_SIZE']\n",
        "PATCH_SIZE = config['PATCH_SIZE']\n",
        "NUM_PATCHES = config['NUM_PATCHES']\n",
        "EMBED_DIM = config['EMBED_DIM']\n",
        "NUM_HEADS = config['NUM_HEADS']\n",
        "MLP_DIM = config['MLP_DIM']\n",
        "ENCODER_LAYERS = config['ENCODER_LAYERS']\n",
        "DECODER_LAYERS = config['DECODER_LAYERS']\n",
        "MAX_LEN = config['MAX_LEN']\n",
        "CONTEXT_DIM = config['CONTEXT_DIM']\n",
        "VOCAB_SIZE = config['VOCAB_SIZE']\n",
        "\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  Context dimension: {CONTEXT_DIM}D\")\n",
        "\n",
        "if CONTEXT_DIM != 180:\n",
        "    print(f\"\\n  WARNING: Model expects {CONTEXT_DIM}D context, but 180D features will be extracted\")\n",
        "    print(f\"  You need to retrain with 180D features if CONTEXT_DIM != 180\")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD TOKENIZER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/5] Loading tokenizer...\")\n",
        "with open(TOKENIZER_PATH, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "idx_to_word = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "idx_to_word[0] = '<pad>'\n",
        "\n",
        "print(f\" Tokenizer loaded ({len(tokenizer.word_index)} words)\")\n",
        "\n",
        "# ============================================================================\n",
        "# OBJECT DETECTOR (80 classes)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3/5] Loading object detector...\")\n",
        "\n",
        "class ObjectDetector:\n",
        "    \"\"\"Detects MSCOCO Objects (80 classes)\"\"\"\n",
        "\n",
        "    def __init__(self, confidence_threshold=0.3, max_objects=20):\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.max_objects = max_objects\n",
        "        self.object_classes = MSCOCO_OBJECTS\n",
        "\n",
        "        self.model = hub.load(\"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\")\n",
        "        print(\"  ✓ Object detector loaded\")\n",
        "\n",
        "    def detect(self, image):\n",
        "        \"\"\"Detect objects and return 80D feature vector (class counts)\"\"\"\n",
        "        if isinstance(image, str):\n",
        "            image = cv2.imread(image)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Run detection\n",
        "        input_tensor = tf.convert_to_tensor(image)[tf.newaxis, ...]\n",
        "        detections = self.model(input_tensor)\n",
        "\n",
        "        boxes = detections['detection_boxes'][0].numpy()\n",
        "        classes = detections['detection_classes'][0].numpy().astype(int)\n",
        "        scores = detections['detection_scores'][0].numpy()\n",
        "\n",
        "        # Count objects by class\n",
        "        class_counts = np.zeros(len(self.object_classes), dtype=np.float32)\n",
        "\n",
        "        valid_indices = np.where(scores >= self.confidence_threshold)[0][:self.max_objects]\n",
        "\n",
        "        detected_objects = []\n",
        "        for idx in valid_indices:\n",
        "            class_id = classes[idx] - 1  # COCO is 1-indexed\n",
        "            if 0 <= class_id < len(self.object_classes):\n",
        "                class_counts[class_id] += 1\n",
        "                detected_objects.append({\n",
        "                    'name': self.object_classes[class_id],\n",
        "                    'confidence': float(scores[idx])\n",
        "                })\n",
        "\n",
        "        # Normalize by max count\n",
        "        max_count = max(class_counts.max(), 1.0)\n",
        "        class_counts = class_counts / max_count\n",
        "\n",
        "        return class_counts, detected_objects\n",
        "\n",
        "# ============================================================================\n",
        "# STUFF DETECTOR (91 classes)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4/5] Loading stuff detector...\")\n",
        "\n",
        "class StuffDetector:\n",
        "    \"\"\"Detects MSCOCO Stuff (91 classes) using heuristics\"\"\"\n",
        "\n",
        "    def __init__(self, coverage_threshold=0.01):\n",
        "        self.coverage_threshold = coverage_threshold\n",
        "        self.stuff_classes = MSCOCO_STUFF\n",
        "        print(\"  ✓ Stuff detector loaded (heuristic-based)\")\n",
        "\n",
        "    def detect(self, image):\n",
        "        \"\"\"Detect stuff and return 91D feature vector\"\"\"\n",
        "        if isinstance(image, str):\n",
        "            image = cv2.imread(image)\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        # Convert to different color spaces\n",
        "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        stuff_features = np.zeros(91, dtype=np.float32)\n",
        "\n",
        "        # Top region for sky\n",
        "        top_region = image[:h//4, :]\n",
        "        blue_ratio = self._detect_color(top_region, 'blue')\n",
        "        stuff_features[0] = min(blue_ratio * 2, 1.0)  # sky\n",
        "\n",
        "        # Bottom region for grass\n",
        "        bottom_region = image[h*3//4:, :]\n",
        "        green_ratio = self._detect_color(bottom_region, 'green')\n",
        "        stuff_features[1] = min(green_ratio * 2, 1.0)  # grass\n",
        "\n",
        "        # Middle region for water\n",
        "        middle_region = image[h//4:h*3//4, :]\n",
        "        water_ratio = self._detect_color(middle_region, 'blue')\n",
        "        stuff_features[6] = min(water_ratio * 1.5, 1.0)  # water\n",
        "\n",
        "        # Sea (blue in lower half)\n",
        "        sea_ratio = self._detect_color(bottom_region, 'blue')\n",
        "        stuff_features[7] = min(sea_ratio * 1.8, 1.0)  # sea\n",
        "\n",
        "        # Tree (green with texture)\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        edge_density = np.sum(edges > 0) / (h * w)\n",
        "        green_all = self._detect_color(image, 'green')\n",
        "        stuff_features[2] = min(green_all * edge_density * 5, 1.0)  # tree\n",
        "\n",
        "        # Building (gray, vertical structures)\n",
        "        gray_ratio = self._detect_color(image, 'gray')\n",
        "        stuff_features[20] = min(gray_ratio * 1.5, 1.0)  # building\n",
        "\n",
        "        # Road (gray in bottom)\n",
        "        road_ratio = self._detect_color(bottom_region, 'gray')\n",
        "        stuff_features[33] = min(road_ratio * 1.5, 1.0)  # road\n",
        "\n",
        "        # Wall\n",
        "        wall_ratio = self._detect_color(middle_region, 'gray')\n",
        "        stuff_features[24] = min(wall_ratio * 1.2, 1.0)  # wall\n",
        "\n",
        "        # Ground (brown/tan)\n",
        "        brown_ratio = self._detect_color(bottom_region, 'brown')\n",
        "        stuff_features[35] = min(brown_ratio * 1.5, 1.0)  # ground\n",
        "\n",
        "        # Sand (light brown/yellow in bottom)\n",
        "        sand_ratio = self._detect_color(bottom_region, 'yellow')\n",
        "        stuff_features[10] = min(sand_ratio * 1.2, 1.0)  # sand\n",
        "\n",
        "        # Snow (white)\n",
        "        white_ratio = self._detect_color(image, 'white')\n",
        "        stuff_features[11] = min(white_ratio * 1.5, 1.0)  # snow\n",
        "\n",
        "        # Mountain (gray/brown in top half)\n",
        "        mountain_ratio = self._detect_color(top_region, 'gray')\n",
        "        stuff_features[3] = min(mountain_ratio * 1.3, 1.0)  # mountain\n",
        "\n",
        "        return stuff_features\n",
        "\n",
        "    def _detect_color(self, region, color):\n",
        "        \"\"\"Detect color presence in image region\"\"\"\n",
        "        hsv = cv2.cvtColor(region, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        color_ranges = {\n",
        "            'blue': ([100, 50, 50], [130, 255, 255]),\n",
        "            'green': ([35, 40, 40], [85, 255, 255]),\n",
        "            'gray': ([0, 0, 50], [180, 50, 200]),\n",
        "            'brown': ([10, 50, 20], [30, 255, 200]),\n",
        "            'yellow': ([20, 100, 100], [35, 255, 255]),\n",
        "            'white': ([0, 0, 200], [180, 50, 255])\n",
        "        }\n",
        "\n",
        "        if color not in color_ranges:\n",
        "            return 0.0\n",
        "\n",
        "        lower, upper = color_ranges[color]\n",
        "        mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n",
        "        ratio = np.sum(mask > 0) / (region.shape[0] * region.shape[1])\n",
        "\n",
        "        return ratio\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE FEATURE EXTRACTOR (180D)\n",
        "# ============================================================================\n",
        "\n",
        "class CompleteFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extracts 180D MSCOCO features:\n",
        "    - 80D: Object counts\n",
        "    - 91D: Stuff coverage\n",
        "    - 9D: Scene statistics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.object_detector = ObjectDetector()\n",
        "        self.stuff_detector = StuffDetector()\n",
        "        print(\"\\n✓ Complete feature extractor initialized (180D)\")\n",
        "\n",
        "    def extract_features(self, image):\n",
        "        \"\"\"Extract complete 180D feature vector\"\"\"\n",
        "        # Objects (80D)\n",
        "        object_features, detected_objects = self.object_detector.detect(image)\n",
        "\n",
        "        # Stuff (91D)\n",
        "        stuff_features = self.stuff_detector.detect(image)\n",
        "\n",
        "        # Scene statistics (9D)\n",
        "        scene_stats = self._compute_scene_stats(object_features, stuff_features)\n",
        "\n",
        "        # Combine\n",
        "        complete_features = np.concatenate([\n",
        "            object_features,  # 80D\n",
        "            stuff_features,   # 91D\n",
        "            scene_stats       # 9D\n",
        "        ])\n",
        "\n",
        "        return complete_features.astype(np.float32), detected_objects\n",
        "\n",
        "    def _compute_scene_stats(self, object_features, stuff_features):\n",
        "        \"\"\"Compute 9D scene statistics\"\"\"\n",
        "        stats = [\n",
        "            np.sum(object_features > 0),           # Number of object types\n",
        "            np.sum(stuff_features > 0),            # Number of stuff types\n",
        "            np.mean(object_features),              # Average object presence\n",
        "            np.std(object_features),               # Object diversity\n",
        "            np.mean(stuff_features),               # Average stuff coverage\n",
        "            np.std(stuff_features),                # Stuff diversity\n",
        "            np.max(object_features),               # Max object count\n",
        "            np.max(stuff_features),                # Max stuff coverage\n",
        "            (np.sum(object_features > 0) + np.sum(stuff_features > 0)) / 171\n",
        "        ]\n",
        "        return np.array(stats, dtype=np.float32)\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[5/5] Building model architecture...\")\n",
        "\n",
        "class PatchExtract(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [tf.shape(images)[0], -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patches, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos = layers.Embedding(num_patches, embed_dim)\n",
        "\n",
        "    def call(self, patches):\n",
        "        pos_ids = tf.range(start=0, limit=NUM_PATCHES, delta=1)\n",
        "        pos_emb = self.pos(pos_ids)\n",
        "        pos_emb = tf.expand_dims(pos_emb, axis=0)\n",
        "        x = self.proj(patches) + pos_emb\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            layers.Dense(mlp_dim, activation=\"gelu\"),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x):\n",
        "        att_out = self.att(x, x)\n",
        "        x = self.ln1(x + att_out)\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + ff_out)\n",
        "        return x\n",
        "\n",
        "class ContextFusion(layers.Layer):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.context_proj = layers.Dense(embed_dim, name=\"context_projection\")\n",
        "        self.dropout = layers.Dropout(0.1)\n",
        "        self.ln = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, enc_out, context, training=False):\n",
        "        context_emb = self.context_proj(context)\n",
        "        context_emb = self.dropout(context_emb, training=training)\n",
        "        context_emb = tf.expand_dims(context_emb, 1)\n",
        "\n",
        "        combined = tf.concat([enc_out, context_emb], axis=1)\n",
        "        return self.ln(combined)\n",
        "\n",
        "class DecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.self_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "        self.cross_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ln3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "\n",
        "    def _causal_mask(self, seq_len, batch_size):\n",
        "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        mask = tf.cast(mask, tf.bool)\n",
        "        mask = tf.reshape(mask, (1, seq_len, seq_len))\n",
        "        return tf.repeat(mask, repeats=batch_size, axis=0)\n",
        "\n",
        "    def call(self, x, enc_out, training=False):\n",
        "        batch = tf.shape(x)[0]\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        mask = self._causal_mask(seq_len, batch)\n",
        "\n",
        "        att1 = self.self_att(x, x, attention_mask=mask)\n",
        "        att1 = self.dropout1(att1, training=training)\n",
        "        x = self.ln1(x + att1)\n",
        "\n",
        "        att2 = self.cross_att(x, enc_out)\n",
        "        att2 = self.dropout2(att2, training=training)\n",
        "        x = self.ln2(x + att2)\n",
        "\n",
        "        f = self.ff(x, training=training)\n",
        "        x = self.ln3(x + f)\n",
        "        return x\n",
        "\n",
        "def build_vit_encoder():\n",
        "    img = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    patches = PatchExtract(PATCH_SIZE)(img)\n",
        "    x = PatchEmbedding(NUM_PATCHES, EMBED_DIM)(patches)\n",
        "\n",
        "    for _ in range(ENCODER_LAYERS):\n",
        "        x = EncoderBlock(EMBED_DIM, NUM_HEADS, MLP_DIM)(x)\n",
        "\n",
        "    return tf.keras.Model(img, x, name=\"vit_encoder\")\n",
        "\n",
        "def build_decoder():\n",
        "    seq_in = layers.Input(shape=(MAX_LEN-1,), dtype=tf.int32)\n",
        "    enc_out = layers.Input(shape=(NUM_PATCHES + 1, EMBED_DIM))\n",
        "\n",
        "    tok_emb = layers.Embedding(VOCAB_SIZE, EMBED_DIM)(seq_in)\n",
        "    pos_ids = tf.range(start=0, limit=MAX_LEN-1, delta=1)\n",
        "    pos_layer = layers.Embedding(MAX_LEN, EMBED_DIM)\n",
        "    pos_emb = pos_layer(pos_ids)\n",
        "    pos_emb = tf.expand_dims(pos_emb, 0)\n",
        "    x = tok_emb + pos_emb\n",
        "\n",
        "    for _ in range(DECODER_LAYERS):\n",
        "        x = DecoderBlock(EMBED_DIM, MLP_DIM, NUM_HEADS)(x, enc_out)\n",
        "\n",
        "    out = layers.Dense(VOCAB_SIZE)(x)\n",
        "    return tf.keras.Model([seq_in, enc_out], out, name=\"decoder\")\n",
        "\n",
        "encoder = build_vit_encoder()\n",
        "decoder = build_decoder()\n",
        "context_fusion = ContextFusion(EMBED_DIM)\n",
        "\n",
        "img_inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"image_input\")\n",
        "context_inp = layers.Input(shape=(CONTEXT_DIM,), name=\"context_input\")\n",
        "seq_inp = layers.Input(shape=(MAX_LEN-1,), dtype=tf.int32, name=\"sequence_input\")\n",
        "\n",
        "enc_out = encoder(img_inp)\n",
        "fused_out = context_fusion(enc_out, context_inp)\n",
        "dec_out = decoder([seq_inp, fused_out])\n",
        "\n",
        "model = tf.keras.Model([img_inp, context_inp, seq_inp], dec_out)\n",
        "\n",
        "print(\"✓ Model architecture built\")\n",
        "\n",
        "print(\"\\nLoading trained weights...\")\n",
        "model.load_weights(WEIGHTS_PATH)\n",
        "print(\"✓ Weights loaded\")\n",
        "\n",
        "# ============================================================================\n",
        "# INITIALIZE FEATURE EXTRACTORS\n",
        "# ============================================================================\n",
        "\n",
        "feature_extractor = CompleteFeatureExtractor()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ ALL SYSTEMS READY - 180D MSCOCO FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# CAPTION GENERATION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_caption(image_path, temperature=1.0, max_length=40, verbose=True):\n",
        "    \"\"\"Generate caption using 180D MSCOCO features\"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"GENERATING CAPTION\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Image: {os.path.basename(image_path)}\")\n",
        "\n",
        "    # Load image\n",
        "    if verbose:\n",
        "        print(\"\\n[1/4] Loading image...\")\n",
        "\n",
        "    img_pil = Image.open(image_path).convert('RGB')\n",
        "    img_array = np.array(img_pil.resize((IMG_SIZE, IMG_SIZE))).astype(np.float32) / 255.0\n",
        "    img_cv = cv2.imread(image_path)\n",
        "\n",
        "    # Extract 180D features\n",
        "    if verbose:\n",
        "        print(\"\\n[2/4] Extracting 180D MSCOCO features...\")\n",
        "        print(\"  - 80D: Object detection\")\n",
        "        print(\"  - 91D: Stuff detection\")\n",
        "        print(\"  - 9D: Scene statistics\")\n",
        "\n",
        "    context_features, detected_objects = feature_extractor.extract_features(img_cv)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n✓ Detected {len(detected_objects)} objects:\")\n",
        "        for obj in detected_objects[:5]:\n",
        "            print(f\"    - {obj['name']}: {obj['confidence']:.2f}\")\n",
        "        if len(detected_objects) > 5:\n",
        "            print(f\"    ... and {len(detected_objects)-5} more\")\n",
        "\n",
        "    # Prepare inputs\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Pad or truncate context to match CONTEXT_DIM\n",
        "    if len(context_features) != CONTEXT_DIM:\n",
        "        if len(context_features) < CONTEXT_DIM:\n",
        "            # Pad with zeros\n",
        "            padding = np.zeros(CONTEXT_DIM - len(context_features), dtype=np.float32)\n",
        "            context_features = np.concatenate([context_features, padding])\n",
        "        else:\n",
        "            # Truncate\n",
        "            context_features = context_features[:CONTEXT_DIM]\n",
        "\n",
        "    context_batch = np.expand_dims(context_features, axis=0)\n",
        "\n",
        "    # Encode image\n",
        "    if verbose:\n",
        "        print(\"\\n[3/4] Encoding image...\")\n",
        "\n",
        "    enc_output = encoder.predict(img_batch, verbose=0)\n",
        "    fused_features = context_fusion(enc_output, context_batch, training=False)\n",
        "\n",
        "    # Generate caption\n",
        "    if verbose:\n",
        "        print(\"\\n[4/4] Generating caption...\")\n",
        "\n",
        "    start_token = tokenizer.word_index.get('startseq', 1)\n",
        "    end_token = tokenizer.word_index.get('endseq', 2)\n",
        "\n",
        "    sequence = [start_token]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [sequence], maxlen=MAX_LEN-1, padding='post'\n",
        "        )\n",
        "\n",
        "        preds = decoder.predict([padded, fused_features], verbose=0)\n",
        "        logits = preds[0, len(sequence)-1, :]\n",
        "        logits = logits / temperature\n",
        "\n",
        "        probs = tf.nn.softmax(logits).numpy()\n",
        "        next_word = np.argmax(probs)\n",
        "\n",
        "        if next_word == end_token or next_word == 0:\n",
        "            break\n",
        "\n",
        "        sequence.append(next_word)\n",
        "\n",
        "    # Convert to caption\n",
        "    caption = ' '.join([idx_to_word.get(idx, '') for idx in sequence[1:]])\n",
        "    caption = caption.replace('startseq', '').replace('endseq', '').strip()\n",
        "\n",
        "    if caption:\n",
        "        caption = caption[0].upper() + caption[1:]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RESULT:\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Caption: {caption}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    return caption\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN USAGE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"USAGE\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\"\"\n",
        "# Generate caption:\n",
        "caption = generate_caption('/content/test_image.jpg')\n",
        "\n",
        "# With more creativity:\n",
        "caption = generate_caption('/content/test_image.jpg', temperature=1.2)\n",
        "\n",
        "# Silent mode:\n",
        "caption = generate_caption('/content/test_image.jpg', verbose=False)\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"READY FOR INFERENCE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nNote: If CONTEXT_DIM != 180, you need to retrain with 180D features\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccFJrLzZXK9R",
        "outputId": "0613e9c9-ad66-401f-db0a-e5d8ac5484d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "AUTOMATIC IMAGE CAPTIONING - 180D MSCOCO Features\n",
            "Loading models and dependencies...\n",
            "================================================================================\n",
            "\n",
            "[1/5] Loading configuration...\n",
            "✓ Configuration loaded\n",
            "  Context dimension: 180D\n",
            "\n",
            "[2/5] Loading tokenizer...\n",
            "✓ Tokenizer loaded (8831 words)\n",
            "\n",
            "[3/5] Loading object detector...\n",
            "\n",
            "[4/5] Loading stuff detector...\n",
            "\n",
            "[5/5] Building model architecture...\n",
            "✓ Model architecture built\n",
            "\n",
            "Loading trained weights...\n",
            "✓ Weights loaded\n",
            "  ✓ Object detector loaded\n",
            "  ✓ Stuff detector loaded (heuristic-based)\n",
            "\n",
            "✓ Complete feature extractor initialized (180D)\n",
            "\n",
            "================================================================================\n",
            "✓ ALL SYSTEMS READY - 180D MSCOCO FEATURES\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "USAGE\n",
            "================================================================================\n",
            "\n",
            "# Generate caption:\n",
            "caption = generate_caption('/content/test_image.jpg')\n",
            "\n",
            "# With more creativity:\n",
            "caption = generate_caption('/content/test_image.jpg', temperature=1.2)\n",
            "\n",
            "# Silent mode:\n",
            "caption = generate_caption('/content/test_image.jpg', verbose=False)\n",
            "    \n",
            "\n",
            "================================================================================\n",
            "READY FOR INFERENCE!\n",
            "================================================================================\n",
            "\n",
            "Note: If CONTEXT_DIM != 180, you need to retrain with 180D features\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption('/content/test_image.jpg')"
      ],
      "metadata": {
        "id": "Gxe_HIMH7FMh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "6cde6f68-1bfd-4d92-ec24-68bde7e9d994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "GENERATING CAPTION\n",
            "================================================================================\n",
            "Image: test_image.jpg\n",
            "\n",
            "[1/4] Loading image...\n",
            "\n",
            "[2/4] Extracting 180D MSCOCO features...\n",
            "  - 80D: Object detection\n",
            "  - 91D: Stuff detection\n",
            "  - 9D: Scene statistics\n",
            "\n",
            "✓ Detected 20 objects:\n",
            "    - person: 0.82\n",
            "    - person: 0.71\n",
            "    - person: 0.65\n",
            "    - person: 0.63\n",
            "    - person: 0.61\n",
            "    ... and 15 more\n",
            "\n",
            "[3/4] Encoding image...\n",
            "\n",
            "[4/4] Generating caption...\n",
            "\n",
            "================================================================================\n",
            "RESULT:\n",
            "================================================================================\n",
            "Caption: A child in the ocean\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A child in the ocean'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLmtX7fd7FdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5qGEHDF7Fg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdywDKCS7Fkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSEx0uEN7Fof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}